{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-02-11 Ready for AI\n",
    "Once the database of RNA-seq values is ready, we can transform it to something that an AI can work with and recognize.\n",
    "\n",
    "The roadmap is as follows:\n",
    "\n",
    "1. **Data preprocessing**: Convert the data to a format that an AI can work with\n",
    "2. **Building the AI**: Find an appropriate network topology, train it, and test it\n",
    "\n",
    "The first step is partially done, but we still need to do some work.\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "We want to transform our data set into the following format: a matrix that has dimensions nsamples * ngenes, where nsamples is the number of samples that we will work with, and ngenes is the number of genes we want to take into consideration.\n",
    "\n",
    "Let's start by building this array.\n",
    "\n",
    "The code that follows will then be migrated to a separate script, which will create a data structure that we will load later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and files\n",
    "tissue_ai_root = \"..\"\n",
    "experiments_dir = \"%s/data/experiments\"%(tissue_ai_root)\n",
    "metadata = \"%s/data/metadata.txt\"%(tissue_ai_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the metadata and develop a dictionary to map a tissue name to a number\n",
    "md = pd.read_csv(metadata, sep='\\t')\n",
    "tissues = md['Biosample term name'].unique()\n",
    "tissues_mapping = {tissues[i] : i for i in range(len(tissues))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = os.listdir(experiments_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the iteration over all the experiments\n",
    "quants = []\n",
    "labels = []\n",
    "n = 0\n",
    "for experiment_name in experiment_names :\n",
    "    \n",
    "    # check that all the replicates from this experiment accession ID have the same tissue\n",
    "    md_experiment = md[md['Experiment accession'] == experiment_name]\n",
    "    tissue_id = md_experiment['Biosample term name'].unique()\n",
    "    if tissue_id.size != 1 :\n",
    "        raise ValueError(\"Experiment %s has replicates from different tissues\"%(experiment_name))\n",
    "    experiment_dir = \"%s/%s\"%(experiments_dir, experiment_name)\n",
    "    \n",
    "    # prepare the iteration over the replicates in the experiment\n",
    "    replicate_n = 1\n",
    "    replicate_dir = \"%s/replicate-%d-quant\"%(experiment_dir, replicate_n)\n",
    "    while os.path.exists(replicate_dir) :\n",
    "        quant_fname = \"%s/quant-by-gene.tsv\"%(replicate_dir)\n",
    "        print(quant_fname)\n",
    "\n",
    "        # increment the number of replicates\n",
    "        replicate_n += 1\n",
    "        replicate_dir = \"%s/replicate-%d-quant\"%(experiment_dir, replicate_n)\n",
    "\n",
    "        # read the file and append it to our list\n",
    "        quant = pd.read_csv(quant_fname, sep='\\t', )\n",
    "        quants.append(quant['GeneTPM'])\n",
    "        labels.append(tissues_mapping[tissue_id[0]])\n",
    "    n += 1\n",
    "    if n == 4 : break\n",
    "df = pd.DataFrame(data = quants, index = pd.RangeIndex(start=0, stop=len(quants)))\n",
    "df['labels'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the AI\n",
    "\n",
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fname = '%s/data/dataset.tsv'%(tissue_ai_root)\n",
    "dataset = np.loadtxt(dataset_fname, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the labels\n",
    "labels = dataset[:, -1]\n",
    "\n",
    "# remove the first column\n",
    "dataset = dataset[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of our input data\n",
    "ngenes = dataset.shape[1]\n",
    "ntissues = len(tissues_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract relevant lines from the data set, and prepare the \"training\", \"validation\", and \"testing\" subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "train_data = dataset[0:1000,:]\n",
    "train_labels = labels[0:1000]\n",
    "\n",
    "# validation\n",
    "valid_data = dataset[1000:1500,:]\n",
    "valid_labels = labels[1000:1500]\n",
    "\n",
    "# testing\n",
    "test_data = dataset[1500:,:]\n",
    "test_labels = labels[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, since we are dealing with a multi-category labelling problem, we will convert our \"labels\" into \"one-hot\" format, which can be interpreted by the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into one-hot format using the \"to_categorical\" function from the keras library\n",
    "train_labels_onehot = keras.utils.to_categorical(train_labels, ntissues)\n",
    "valid_labels_onehot = keras.utils.to_categorical(valid_labels, ntissues)\n",
    "test_labels_onehot = keras.utils.to_categorical(test_labels, ntissues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build the neural network. The first simple tentative model will be to try to build a simple multi-layer perceptron (MLP) architecture, and see whether the results will be interesting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(ngenes,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(ntissues, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our model with our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have currently almost 20 million trainable parameters!\n",
    "\n",
    "We can now compile our model and provide it with a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model will have a loss function based on the categorical crossentropy, and a\n",
    "# RMSprop optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to go for the training. We will use a checkpointer to keep track of what were the best parameters that were found during the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='../data/weights.best.MLP.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "history = model.fit(train_data, train_labels_onehot,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    validation_data=(valid_data, valid_labels_onehot),\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_data, test_labels_onehot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy of 0.4%, which is close to random (1/223 = 0.0046). The most likely explanation for this is that we don't have enough data: we have only ~1700 samples, which means that for each category we have less than 10 data points. It is unlikely that an AI can pick up patterns with such a small data set.\n",
    "\n",
    "One thing that we can try is to reduce drastically the number of categories. If we have something like 10 categories in total, we might get some better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "vpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
